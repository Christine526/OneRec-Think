#!/usr/bin/env python3
"""
Klingæ•°æ® - Itemic Dense Captioning (ç‰©å“æè¿°) è®­ç»ƒæ•°æ®ç”Ÿæˆè„šæœ¬

å¯¹åº”è®ºæ–‡é™„å½•A.3.1ä¸­çš„ä»»åŠ¡3ï¼šItemic Dense Captioning
ä¸ºç»™å®šçš„ç‰©å“SIDç”Ÿæˆæ–‡æœ¬æè¿°ï¼ŒåŒ…å«æ ‡é¢˜å’Œç±»åˆ«ä¿¡æ¯

è¾“å…¥æ–‡ä»¶:
- kling_items.json: ç‰©å“å…ƒæ•°æ®
- kling_sequential.txt: ç”¨æˆ·è¡Œä¸ºåºåˆ—

è¾“å‡ºæ–‡ä»¶:
- training_align_data_train.parquet
- training_align_data_val.parquet
- training_align_data_test.parquet
"""

from __future__ import annotations

import json
from pathlib import Path

import pandas as pd


def load_kling_items(items_file: Path) -> dict:
    """åŠ è½½Klingç‰©å“å…ƒæ•°æ®"""
    print(f"åŠ è½½ç‰©å“å…ƒæ•°æ®: {items_file}")
    with items_file.open("r", encoding="utf-8") as f:
        items = json.load(f)
    print(f"ç‰©å“æ•°é‡: {len(items):,}")
    return items


def build_item_descriptions(
    item_ids: list[str],
    user_id: str,
    kling_items: dict
) -> tuple[list[str], int]:
    """
    æ„å»ºç‰©å“æè¿°åˆ—è¡¨
    
    Args:
        item_ids: ç‰©å“IDåˆ—è¡¨
        user_id: ç”¨æˆ·ID
        kling_items: ç‰©å“å…ƒæ•°æ®å­—å…¸
    
    Returns:
        (ç‰©å“æè¿°åˆ—è¡¨, è·³è¿‡çš„ç‰©å“æ•°é‡)
    """
    item_descriptions = []
    skipped_count = 0
    
    for item_id in item_ids:
        item_info = kling_items.get(item_id)
        if not item_info:
            skipped_count += 1
            continue
        
        sid = item_info.get('sid', '')
        title = item_info.get('title', '')
        categories = item_info.get('categories', '')
        
        # å¿…é¡»æœ‰SIDã€æ ‡é¢˜å’Œç±»åˆ«
        if not (sid and title and categories):
            skipped_count += 1
            continue
        
        # æ„å»ºæè¿°ï¼šSID + æ ‡é¢˜ + ç±»åˆ«
        item_desc = f'{sid}, its title is "{title}", its categories are "{categories}"'
        item_descriptions.append(item_desc)
    
    return item_descriptions, skipped_count


def generate_kling_training_data(
    sequential_file: Path,
    items_file: Path,
    output_train: Path,
    output_val: Path,
    output_test: Path,
) -> None:
    """ç”ŸæˆKlingç‰©å“æè¿°è®­ç»ƒæ•°æ®"""
    
    # åŠ è½½æ•°æ®
    kling_items = load_kling_items(items_file)
    
    print(f"\nåŠ è½½ç”¨æˆ·åºåˆ—æ–‡ä»¶: {sequential_file}")
    with sequential_file.open("r", encoding="utf-8") as f:
        lines = f.readlines()
    print(f"åºåˆ—æ•°é‡: {len(lines):,}")
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    print("\nç”Ÿæˆè®­ç»ƒæ•°æ®...")
    train_rows: list[dict] = []
    val_rows: list[dict] = []
    test_rows: list[dict] = []
    
    total_items_skipped = 0
    
    for idx, line in enumerate(lines):
        line = line.strip()
        if not line:
            continue
        
        elements = line.split()
        if len(elements) <= 1:
            continue
        
        user_id = elements[0]
        item_ids = elements[1:]
        
        # æµ‹è¯•é›†ï¼šä½¿ç”¨å®Œæ•´åºåˆ—
        full_descriptions, skipped_full = build_item_descriptions(
            item_ids, user_id, kling_items
        )
        total_items_skipped += skipped_full
        
        if full_descriptions:
            test_description = "The user has interacted with the following items: " + "; ".join(full_descriptions) + ";"
            test_rows.append({
                'user_id': user_id,
                'description': test_description
            })
        
        # éªŒè¯é›†ï¼šç§»é™¤æœ€å1ä¸ªç‰©å“
        if len(item_ids) > 1:
            val_item_ids = item_ids[:-1]
            val_descriptions, skipped_val = build_item_descriptions(
                val_item_ids, user_id, kling_items
            )
            total_items_skipped += skipped_val
            
            if val_descriptions:
                val_description = "The user has interacted with the following items: " + "; ".join(val_descriptions) + ";"
                val_rows.append({
                    'user_id': user_id,
                    'description': val_description
                })
        
        # è®­ç»ƒé›†ï¼šç§»é™¤æœ€å2ä¸ªç‰©å“
        if len(item_ids) > 2:
            train_item_ids = item_ids[:-2]
            train_descriptions, skipped_train = build_item_descriptions(
                train_item_ids, user_id, kling_items
            )
            total_items_skipped += skipped_train
            
            if train_descriptions:
                train_description = "The user has interacted with the following items: " + "; ".join(train_descriptions) + ";"
                train_rows.append({
                    'user_id': user_id,
                    'description': train_description
                })
        
        if (idx + 1) % 1000 == 0:
            print(f"  å·²å¤„ç† {idx + 1:,} ä¸ªç”¨æˆ·...")
    
    print(f"\nå¤„ç†å®Œæˆ:")
    print(f"  æ€»å…±è·³è¿‡: {total_items_skipped:,} ä¸ªç‰©å“ (ç¼ºå¤±å…ƒæ•°æ®)")
    
    # åˆ›å»ºDataFrame
    print("\nåˆ›å»ºDataFrame...")
    df_train = pd.DataFrame(train_rows)
    df_val = pd.DataFrame(val_rows)
    df_test = pd.DataFrame(test_rows)
    
    print(f"\nâœ… æ•°æ®é›†å¤§å°:")
    print(f"  è®­ç»ƒé›†: {len(df_train):,} æ¡")
    print(f"  éªŒè¯é›†: {len(df_val):,} æ¡")
    print(f"  æµ‹è¯•é›†: {len(df_test):,} æ¡")
    
    # ä¿å­˜æ–‡ä»¶
    print(f"\nä¿å­˜è®­ç»ƒé›†åˆ°: {output_train}")
    df_train.to_parquet(output_train, engine="pyarrow", index=False)
    
    print(f"ä¿å­˜éªŒè¯é›†åˆ°: {output_val}")
    df_val.to_parquet(output_val, engine="pyarrow", index=False)
    
    print(f"ä¿å­˜æµ‹è¯•é›†åˆ°: {output_test}")
    df_test.to_parquet(output_test, engine="pyarrow", index=False)
    
    # é¢„è§ˆæ•°æ®
    def preview(df: pd.DataFrame, name: str) -> None:
        if len(df) == 0:
            return
        print(f"\nğŸ“‹ {name} å‰2æ¡æ ·æœ¬é¢„è§ˆ:")
        for i, (_, row) in enumerate(df.head(2).iterrows()):
            print(f"\n  æ ·æœ¬ {i+1}:")
            print(f"    ç”¨æˆ·ID: {row['user_id']}")
            desc = row['description']
            if len(desc) > 300:
                print(f"    æè¿°: {desc[:300]}...")
            else:
                print(f"    æè¿°: {desc}")
    
    preview(df_train, "è®­ç»ƒé›†")
    preview(df_val, "éªŒè¯é›†")
    
    print("\nâœ¨ æ•°æ®ç”Ÿæˆå®Œæˆ!")


if __name__ == "__main__":
    # æ–‡ä»¶è·¯å¾„
    sequential_file = Path("./kling_sequential.txt")
    items_file = Path("./kling_items.json")
    
    output_train = Path("./training_align_data_train.parquet")
    output_val = Path("./training_align_data_val.parquet")
    output_test = Path("./training_align_data_test.parquet")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    for file in [sequential_file, items_file]:
        if not file.exists():
            print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {file}")
            exit(1)
    
    # ç”Ÿæˆæ•°æ®
    generate_kling_training_data(
        sequential_file=sequential_file,
        items_file=items_file,
        output_train=output_train,
        output_val=output_val,
        output_test=output_test,
    )
