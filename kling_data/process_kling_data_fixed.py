#!/usr/bin/env python3
"""
Klingæ•°æ®é¢„å¤„ç†è„šæœ¬ (ä¿®æ­£ç‰ˆ)
å°†kling_data.csvè½¬æ¢ä¸ºOneRec-Thinkéœ€è¦çš„æ ¼å¼

ä¸»è¦ä¿®æ­£ï¼š
1. âœ… ä¿®æ­£SIDæ ¼å¼ï¼šä»3ç»´æ‰©å±•ä¸º4ç»´ (s_a, s_b, s_c, s_d)
2. âœ… ä½¿ç”¨content_typeä½œä¸ºç¬¬4ç»´ï¼Œä¿ç•™ä¸šåŠ¡è¯­ä¹‰
3. âœ… å®Œå–„é”™è¯¯å¤„ç†å’Œæ•°æ®éªŒè¯
"""

import pandas as pd
import json
from pathlib import Path
import ast
import warnings
warnings.filterwarnings('ignore')


def clean_column_names(df):
    """æ¸…ç†åˆ—åï¼Œç§»é™¤å‰ç¼€"""
    df.columns = [col.split('.')[-1] for col in df.columns]
    return df


def parse_semantic_id(semantic_id_str):
    """
    è§£æsemantic_idå­—ç¬¦ä¸²ä¸ºæ•°ç»„
    ä¾‹å¦‚: ' [69,142,246] ' -> [69, 142, 246]
    """
    if pd.isna(semantic_id_str) or semantic_id_str == '':
        return None
    try:
        # å»é™¤ç©ºæ ¼å¹¶è§£æ
        semantic_id_str = str(semantic_id_str).strip()
        result = ast.literal_eval(semantic_id_str)
        # ç¡®ä¿æ˜¯åˆ—è¡¨ä¸”è‡³å°‘æœ‰3ä¸ªå…ƒç´ 
        if isinstance(result, list) and len(result) >= 3:
            return result
        return None
    except:
        return None


def format_semantic_id_to_sid(semantic_id_array, content_type=0):
    """
    å°†3ç»´semantic_idæ‰©å±•ä¸º4ç»´SIDæ ¼å¼ï¼ˆOneRec-Thinkè¦æ±‚ï¼‰
    
    ä¿®æ­£è¦ç‚¹ï¼š
    - åŸç‰ˆä½¿ç”¨3ç»´ï¼š<s_a><s_b><s_c>
    - ä¿®æ­£åä½¿ç”¨4ç»´ï¼š<s_a><s_b><s_c><s_d>
    - ç¬¬4ç»´ä½¿ç”¨content_typeï¼Œæœ‰ä¸šåŠ¡å«ä¹‰ï¼ˆ0=å›¾ç‰‡+è§†é¢‘, 1=å›¾ç‰‡, 2=è§†é¢‘ï¼‰
    
    Args:
        semantic_id_array: [a, b, c] 3ç»´è¯­ä¹‰ID
        content_type: å†…å®¹ç±»å‹ (0/1/2)
    
    Returns:
        ä¾‹å¦‚: [69, 142, 246] + content_type=0 -> 
             <|sid_begin|><s_a_69><s_b_142><s_c_246><s_d_0><|sid_end|>
    """
    if not semantic_id_array or len(semantic_id_array) < 3:
        return None
    
    # å–å‰3ä¸ªå€¼
    a, b, c = semantic_id_array[0:3]
    
    # éªŒè¯èŒƒå›´ï¼ˆOneRec-Thinkä½¿ç”¨0-255ï¼‰
    if not all(0 <= x <= 255 for x in [a, b, c]):
        print(f"âš ï¸  è­¦å‘Š: semantic_idå€¼è¶…å‡ºèŒƒå›´[0,255]: [{a}, {b}, {c}]")
        return None
    
    # ç¬¬4ç»´ä½¿ç”¨content_type (0/1/2)
    if pd.notna(content_type):
        try:
            d = int(content_type)
            # ç¡®ä¿dä¹Ÿåœ¨åˆç†èŒƒå›´
            if d < 0 or d > 255:
                d = 0
        except:
            d = 0
    else:
        d = 0
    
    # æ„é€ 4ç»´SIDï¼ˆç¬¦åˆOneRec-Thinkæ ¼å¼ï¼‰
    sid = f'<|sid_begin|><s_a_{a}><s_b_{b}><s_c_{c}><s_d_{d}><|sid_end|>'
    return sid


def extract_category(row):
    """
    ä»æ•°æ®ä¸­æå–ç±»åˆ«ä¿¡æ¯
    content_type: 0=å›¾ç‰‡+è§†é¢‘, 1=å›¾ç‰‡, 2=è§†é¢‘
    kling_photo_type: 1=ç´ æ, 2=çŸ­ç‰‡
    """
    # åŸºäºcontent_type
    if pd.notna(row.get('content_type')):
        try:
            content_type = int(float(row['content_type']))
            content_type_map = {
                0: 'Image and Video Creation',
                1: 'Image Creation',
                2: 'Video Creation'
            }
            category = content_type_map.get(content_type, f'Content Type {content_type}')
        except:
            category = 'General Creation'
    else:
        category = 'General Creation'
    
    # æ·»åŠ photo_typeä¿¡æ¯
    if pd.notna(row.get('kling_photo_type')):
        try:
            photo_type = int(float(row['kling_photo_type']))
            if photo_type == 1:
                category += ' > Material'  # ç´ æ
            elif photo_type == 2:
                category += ' > Short Film'  # çŸ­ç‰‡
            else:
                category += f' > Type {photo_type}'
        except:
            pass
    
    return category


def create_item_metadata(df_raw, output_path):
    """
    åˆ›å»ºç‰©å“å…ƒæ•°æ®JSON (ç±»ä¼¼Beauty.pretrain.json)
    
    æ ¼å¼è¦æ±‚ï¼š
    {
        "item_id": {
            "title": "...",
            "description": "...",
            "categories": "...",
            "sid": "<|sid_begin|><s_a_X><s_b_X><s_c_X><s_d_X><|sid_end|>"  # 4ç»´ï¼
        }
    }
    """
    print("="*60)
    print("ğŸ“Š åˆ›å»ºç‰©å“å…ƒæ•°æ®...")
    print("="*60)
    
    # ç»Ÿè®¡è¡Œä¸ºç±»å‹åˆ†å¸ƒï¼ˆevent_type: RECOMMEND/SEARCH/PRODUCEï¼‰
    if 'event_type' in df_raw.columns:
        event_counts = df_raw['event_type'].value_counts()
        print("\nğŸ“Œ è¡Œä¸ºç±»å‹åˆ†å¸ƒ:")
        
        # æŒ‰ç±»å‹æ’åºæ˜¾ç¤º
        event_order = ['RECOMMEND', 'SEARCH', 'PRODUCE']
        for event_type in event_order:
            if event_type in event_counts:
                count = event_counts[event_type]
                pct = count/len(df_raw)*100
                print(f"  {event_type:12s}: {count:5d} æ¡ ({pct:5.1f}%)")
    
    # ç»Ÿè®¡æœç´¢æŸ¥è¯¢å†…å®¹ï¼ˆä»…ç»Ÿè®¡å±•ç¤ºï¼Œæš‚æœªç”¨äºè®­ç»ƒï¼‰
    if 'element_query_content' in df_raw.columns:
        search_with_query = df_raw['element_query_content'].notna().sum()
        total_search = (df_raw['event_type'] == 'SEARCH').sum() if 'event_type' in df_raw.columns else 0
        
        print("\nğŸ” æœç´¢åœºæ™¯ç»Ÿè®¡ (element_query_content):")
        if total_search > 0:
            print(f"  SEARCHç±»å‹æ€»æ•°: {total_search} æ¡")
            print(f"  åŒ…å«æŸ¥è¯¢è¯: {search_with_query} æ¡ ({search_with_query/total_search*100:.1f}%)")
        else:
            print(f"  åŒ…å«æŸ¥è¯¢è¯çš„è®°å½•: {search_with_query} æ¡")
    
    # æŒ‰kling_photo_idå»é‡ï¼Œä¿ç•™æœ€æ–°çš„è®°å½•
    df_items = df_raw.sort_values('time_stamp').drop_duplicates(
        subset=['kling_photo_id'], keep='last'
    )
    
    print(f"\nğŸ“¦ ç‰©å“å»é‡:")
    print(f"  å»é‡å‰: {len(df_raw)} æ¡è®°å½•")
    print(f"  å»é‡å: {len(df_items)} ä¸ªå”¯ä¸€ç‰©å“")
    
    items_dict = {}
    missing_sid_count = 0
    invalid_sid_count = 0
    
    for idx, row in df_items.iterrows():
        item_id = str(row['kling_photo_id'])
        
        # è§£æsemantic_id
        semantic_id_array = parse_semantic_id(row['semantic_id'])
        if not semantic_id_array:
            missing_sid_count += 1
            continue
        
        # è½¬æ¢ä¸º4ç»´SIDæ ¼å¼ï¼ˆå…³é”®ä¿®æ­£ï¼‰
        content_type = row.get('content_type', 0)
        sid = format_semantic_id_to_sid(semantic_id_array, content_type)
        if not sid:
            invalid_sid_count += 1
            continue
        
        # æå–ç±»åˆ«
        categories = extract_category(row)
        
        description = row['prompt'] if pd.notna(row['prompt']) else ''
        # æ·»åŠ é•¿åº¦é™åˆ¶ï¼Œé¿å…è¶…é•¿æ–‡æœ¬, QWençš„tokené™åˆ¶ä¸ºmax_length=4096
        if len(description) > 1000:  # é™åˆ¶å­—ç¬¦æ•°
            description = description[:1000] + '...'
        
        # ä½¿ç”¨titleæˆ–introductionä½œä¸ºæ ‡é¢˜
        title = row['title'] if pd.notna(row['title']) else ''
        if not title and pd.notna(row['introduction']):
            title = row['introduction'][:100]  # æˆªå–å‰100å­—ç¬¦
        if not title:
            title = description[:100] if description else f'Item {item_id}'
        
        items_dict[item_id] = {
            'title': title,
            'description': description,
            'categories': categories,
            'sid': sid
        }
    
    print(f"\nâœ… å¤„ç†ç»“æœ:")
    print(f"  æˆåŠŸå¤„ç†: {len(items_dict)} ä¸ªç‰©å“")
    print(f"  ç¼ºå¤±SID: {missing_sid_count} ä¸ªç‰©å“")
    print(f"  æ— æ•ˆSID: {invalid_sid_count} ä¸ªç‰©å“")
    
    # éªŒè¯SIDæ ¼å¼
    sample_sids = list(items_dict.values())[:3]
    print(f"\nğŸ” SIDæ ¼å¼éªŒè¯ï¼ˆå‰3ä¸ªæ ·ä¾‹ï¼‰:")
    for i, item in enumerate(sample_sids):
        sid = item['sid']
        # æ£€æŸ¥æ˜¯å¦åŒ…å«4ä¸ªç»´åº¦
        has_4_dims = all(f'<s_{dim}_' in sid for dim in ['a', 'b', 'c', 'd'])
        status = "âœ…" if has_4_dims else "âŒ"
        print(f"  {status} [{i+1}] {sid}")
    
    # ä¿å­˜ä¸ºJSON
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(items_dict, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ç‰©å“å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    
    return items_dict


def create_sequential_file(df_raw, output_path, min_sequence_length=3):
    """
    åˆ›å»ºç”¨æˆ·åºåˆ—æ–‡ä»¶ (ç±»ä¼¼sequential_data_processed.txt)
    
    æ ¼å¼: user_id item_id1 item_id2 item_id3 ...
    
    æ³¨æ„: å‡è®¾CSVä¸­å·²ç»åŒ…å«æ­£å‘è¡Œä¸ºï¼ˆåœ¨SQLä¸­å·²ç­›é€‰ï¼‰
    
    Args:
        df_raw: æ­£å‘è¡Œä¸ºæ•°æ®DataFrame
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        min_sequence_length: æœ€å°åºåˆ—é•¿åº¦
    """
    print("\n" + "="*60)
    print("ğŸ“ åˆ›å»ºç”¨æˆ·åºåˆ—æ–‡ä»¶...")
    print("="*60)
    
    print(f"æ€»è®°å½•æ•°: {len(df_raw)} æ¡")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„semantic_id
    df_raw['semantic_id_parsed'] = df_raw['semantic_id'].apply(parse_semantic_id)
    df_positive = df_raw[df_raw['semantic_id_parsed'].notna()].copy()
    
    print(f"æœ‰æ•ˆsemantic_idè®°å½•: {len(df_positive)} æ¡")
    
    # æŒ‰ç”¨æˆ·å’Œæ—¶é—´æ’åº
    df_sorted = df_positive.sort_values(['user_id', 'time_stamp'])
    
    # ç»Ÿè®¡ä¿¡æ¯
    user_counts = df_sorted.groupby('user_id').size()
    total_users = len(user_counts)
    valid_users = (user_counts >= min_sequence_length).sum()
    
    print(f"\nğŸ“Š ç”¨æˆ·ç»Ÿè®¡:")
    print(f"  æ€»ç”¨æˆ·æ•°: {total_users}")
    print(f"  åºåˆ—é•¿åº¦ >= {min_sequence_length} çš„ç”¨æˆ·: {valid_users}")
    print(f"  å¹³å‡åºåˆ—é•¿åº¦: {user_counts.mean():.2f}")
    print(f"  ä¸­ä½åºåˆ—é•¿åº¦: {user_counts.median():.0f}")
    print(f"  æœ€é•¿åºåˆ—: {user_counts.max()}")
    
    # å†™å…¥åºåˆ—æ–‡ä»¶
    written_count = 0
    with open(output_path, 'w', encoding='utf-8') as f:
        for user_id, group in df_sorted.groupby('user_id'):
            item_ids = group['kling_photo_id'].astype(str).tolist()
            if len(item_ids) >= min_sequence_length:
                line = f"{user_id} {' '.join(item_ids)}\n"
                f.write(line)
                written_count += 1
    
    print(f"\nâœ… æˆåŠŸå†™å…¥ {written_count} ä¸ªç”¨æˆ·åºåˆ—")
    print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_path}")
    
    return valid_users


def create_user_behaviors_file(df_raw, output_path):
    """
    åˆ›å»ºç”¨æˆ·è¡Œä¸ºè¯¦ç»†ä¿¡æ¯æ–‡ä»¶ï¼ŒåŒ…å«æ¯ä¸ªäº¤äº’çš„è¡Œä¸ºç±»å‹
    è¿™ä¸ªæ–‡ä»¶ç”¨äºç”Ÿæˆå¯Œæ–‡æœ¬çš„Alignmentè®­ç»ƒæ•°æ®
    
    Args:
        df_raw: åŸå§‹è¡Œä¸ºæ•°æ®DataFrame
        output_path: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
    """
    print("\n" + "="*60)
    print("ğŸ“‹ åˆ›å»ºç”¨æˆ·è¡Œä¸ºè¯¦æƒ…æ–‡ä»¶...")
    print("="*60)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„semantic_id
    df_raw['semantic_id_parsed'] = df_raw['semantic_id'].apply(parse_semantic_id)
    df_positive = df_raw[df_raw['semantic_id_parsed'].notna()].copy()
    
    # æŒ‰ç”¨æˆ·å’Œæ—¶é—´æ’åº
    df_sorted = df_positive.sort_values(['user_id', 'time_stamp'])
    
    user_behaviors = {}
    
    for user_id, group in df_sorted.groupby('user_id'):
        behaviors = []
        
        for _, row in group.iterrows():
            behavior_info = {
                'item_id': str(row['kling_photo_id']),
                'event_type': row['event_type'] if pd.notna(row.get('event_type')) else 'UNKNOWN',
                'behavior_type': row['behavior_type'] if pd.notna(row.get('behavior_type')) else '',
                'behavior_subtype': row['behavior_subtype'] if pd.notna(row.get('behavior_subtype')) else '',
                'timestamp': str(row['time_stamp']) if pd.notna(row.get('time_stamp')) else '',
                'element_query_content': row['element_query_content'] if pd.notna(row.get('element_query_content')) else '',
                'query_cnt': int(row['query_cnt']) if pd.notna(row.get('query_cnt')) else 0
            }
            behaviors.append(behavior_info)
        
        user_behaviors[str(user_id)] = behaviors
    
    # ä¿å­˜ä¸ºJSON
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(user_behaviors, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ€»ç”¨æˆ·æ•°: {len(user_behaviors)}")
    print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_path}")
    
    return user_behaviors


def main():
    print("\n" + "ğŸš€"*30)
    print("Klingæ•°æ®å¤„ç†è„šæœ¬ (OneRec-Thinkæ ¼å¼)")
    print("ä¿®æ­£ç‰ˆæœ¬ï¼šæ”¯æŒ4ç»´SIDæ ¼å¼")
    print("ğŸš€"*30 + "\n")
    
    # æ–‡ä»¶è·¯å¾„ï¼ˆæŒ‰ä¼˜å…ˆçº§æ£€æŸ¥ï¼‰
    possible_files = [
        Path("./kling_data_fixed.tsv"),  # ä¿®å¤åçš„TSVï¼ˆä¼˜å…ˆï¼‰
        Path("./kling_data.tsv"),        # TSVæ ¼å¼ï¼ˆåˆ¶è¡¨ç¬¦åˆ†éš”ï¼‰
        Path("./kling_data_fixed.csv"),  # ä¿®å¤åçš„CSVï¼ˆå¤‡é€‰ï¼‰
        Path("./kling_data.csv"),        # CSVæ ¼å¼ï¼ˆé€—å·åˆ†éš”ï¼Œå¤‡é€‰ï¼‰
    ]
    
    input_file = None
    for f in possible_files:
        if f.exists():
            input_file = f
            print(f"âœ… æ£€æµ‹åˆ°æ•°æ®æ–‡ä»¶: {input_file}\n")
            break
    
    items_output = Path("./kling_items.json")
    sequential_output = Path("./kling_sequential.txt")
    behaviors_output = Path("./kling_user_behaviors.json")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if input_file is None:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶")
        print(f"\nè¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶ä¹‹ä¸€å­˜åœ¨:")
        for f in possible_files:
            print(f"  - {f}")
        print(f"\næç¤ºï¼šå¦‚æœä»Hiveå¯¼å‡ºæ•°æ®ï¼Œè¯·ä½¿ç”¨åˆ¶è¡¨ç¬¦(\\t)ä½œä¸ºåˆ†éš”ç¬¦")
        return
    
    # è¯»å–CSV (å°è¯•å¤šç§åˆ†éš”ç¬¦å’Œç¼–ç )
    print(f"ğŸ“– è¯»å–æ•°æ®æ–‡ä»¶: {input_file}")
    df = None
    
    # å°è¯•å¤šç§è¯»å–æ–¹å¼
    attempts = [
        # 1. åˆ¶è¡¨ç¬¦åˆ†éš”ï¼ˆæœ€å¸¸è§çš„è§£å†³æ–¹æ¡ˆï¼‰
        {'sep': '\t', 'encoding': 'utf-8-sig', 'engine': 'python', 'desc': 'åˆ¶è¡¨ç¬¦åˆ†éš”'},
        # 2. æ ‡å‡†CSV
        {'sep': ',', 'encoding': 'utf-8-sig', 'desc': 'æ ‡å‡†CSV'},
        # 3. Pythonå¼•æ“ + åˆ¶è¡¨ç¬¦
        {'sep': '\t', 'encoding': 'utf-8-sig', 'engine': 'python', 'on_bad_lines': 'skip', 'desc': 'åˆ¶è¡¨ç¬¦(è·³è¿‡åè¡Œ)'},
        # 4. Pythonå¼•æ“ + é€—å·ï¼ˆå®½æ¾æ¨¡å¼ï¼‰
        {'sep': ',', 'encoding': 'utf-8-sig', 'engine': 'python', 'on_bad_lines': 'skip', 'quoting': 3, 'desc': 'é€—å·åˆ†éš”(è·³è¿‡åè¡Œ)'},
    ]
    
    for i, params in enumerate(attempts, 1):
        desc = params.pop('desc')
        try:
            print(f"  å°è¯•æ–¹å¼ {i}: {desc}")
            df = pd.read_csv(input_file, **params)
            print(f"  âœ… æˆåŠŸ! è¯»å– {len(df)} æ¡è®°å½•ï¼Œ{len(df.columns)} åˆ—")
            break
        except (pd.errors.ParserError, TypeError) as e:
            print(f"    âŒ å¤±è´¥: {str(e)[:50]}...")
            continue
        except Exception as e:
            print(f"    âŒ å¤±è´¥: {str(e)[:50]}...")
            continue
    
    # å¦‚æœæ‰€æœ‰æ–¹å¼éƒ½å¤±è´¥
    if df is None or len(df) == 0:
        print(f"\nâŒ é”™è¯¯: æ— æ³•è¯»å–æ•°æ®æ–‡ä»¶")
        print(f"è¯·ä½¿ç”¨åˆ¶è¡¨ç¬¦(\\t)é‡æ–°å¯¼å‡ºæ•°æ®")
        return
    
    # ç§»é™¤BOMï¼ˆå¦‚æœæœ‰ï¼‰
    if df.columns[0].startswith('\ufeff'):
        df.columns = [col.replace('\ufeff', '') for col in df.columns]
        print(f"  âœ… ç§»é™¤äº†UTF-8 BOMæ ‡è®°")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¡¨å¤´ï¼ˆå¦‚æœç¬¬ä¸€è¡Œå…¨æ˜¯æ•°å­—ï¼Œè¯´æ˜æ²¡æœ‰è¡¨å¤´ï¼‰
    first_row = df.iloc[0]
    if all(str(val).replace('.', '').replace('-', '').replace(':', '').replace(' ', '').isdigit() or pd.isna(val) for val in first_row[:3]):
        print(f"  âš ï¸  æ£€æµ‹åˆ°æ–‡ä»¶å¯èƒ½æ²¡æœ‰è¡¨å¤´ï¼Œè®¾ç½®æ ‡å‡†åˆ—å...")
        expected_columns = [
            'user_id', 'kling_photo_id', 'kling_photo_type', 'event_type',
            'behavior_type', 'behavior_subtype', 'time_stamp', 'content_type',
            'prompt', 'title', 'introduction', 'element_query_content',
            'query_cnt', 'semantic_id'
        ]
        if len(df.columns) == len(expected_columns):
            df.columns = expected_columns
            print(f"  âœ… å·²è®¾ç½®æ ‡å‡†åˆ—å")
        else:
            print(f"  âš ï¸  åˆ—æ•°ä¸åŒ¹é…: æœŸæœ› {len(expected_columns)}ï¼Œå®é™… {len(df.columns)}")
    
    # æ¸…ç†åˆ—å
    df = clean_column_names(df)
    
    print(f"\nğŸ“‹ æ•°æ®åˆ—: {list(df.columns)}")
    
    # æ­¥éª¤1: åˆ›å»ºç‰©å“å…ƒæ•°æ®
    items_dict = create_item_metadata(df, items_output)
    
    # æ­¥éª¤2: åˆ›å»ºç”¨æˆ·åºåˆ—
    valid_users = create_sequential_file(df, sequential_output)
    
    # æ­¥éª¤3: åˆ›å»ºç”¨æˆ·è¡Œä¸ºè¯¦æƒ…ï¼ˆç”¨äºç”Ÿæˆå¯Œæ–‡æœ¬Alignmentæ•°æ®ï¼‰
    user_behaviors = create_user_behaviors_file(df, behaviors_output)
    
    print(f"\n" + "="*60)
    print(f"âœ… æ•°æ®å¤„ç†å®Œæˆ!")
    print(f"="*60)
    print(f"\nğŸ“¦ è¾“å‡ºæ–‡ä»¶:")
    print(f"  1. ç‰©å“å…ƒæ•°æ®: {items_output}")
    print(f"     - å…± {len(items_dict)} ä¸ªç‰©å“")
    print(f"     - æ ¼å¼: OneRec-Thinkå…¼å®¹ (4ç»´SID)")
    print(f"\n  2. ç”¨æˆ·åºåˆ—: {sequential_output}")
    print(f"     - å…± {valid_users} ä¸ªç”¨æˆ·")
    print(f"     - æ ¼å¼: user_id item_id1 item_id2 ...")
    print(f"\n  3. ç”¨æˆ·è¡Œä¸ºè¯¦æƒ…: {behaviors_output}")
    print(f"     - å…± {len(user_behaviors)} ä¸ªç”¨æˆ·")
    print(f"     - åŒ…å«è¡Œä¸ºç±»å‹ã€æ—¶é—´æˆ³ç­‰è¯¦ç»†ä¿¡æ¯")
    
    print(f"\nğŸ¯ ä¸‹ä¸€æ­¥:")
    print(f"  1. éªŒè¯SIDæ ¼å¼: æ£€æŸ¥ {items_output} ä¸­çš„SIDæ˜¯å¦ä¸º4ç»´")
    print(f"  2. ç”Ÿæˆè®­ç»ƒæ•°æ®: è¿è¡Œ data/generate_*_data_kling.py")
    print(f"  3. å¼€å§‹è®­ç»ƒ: è¿è¡Œ train/run_training_stage*.sh")
    print(f"\nè¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹: kling_data_analysis.md\n")


if __name__ == "__main__":
    main()
