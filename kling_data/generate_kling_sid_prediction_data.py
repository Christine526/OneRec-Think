#!/usr/bin/env python3
"""
Klingæ•°æ® - Sequential Preference Modeling (SIDé¢„æµ‹) è®­ç»ƒæ•°æ®ç”Ÿæˆè„šæœ¬

å¯¹åº”è®ºæ–‡é™„å½•A.3.1ä¸­çš„ä»»åŠ¡2ï¼šSequential Preference Modeling
æ ¹æ®ç”¨æˆ·å†å²äº¤äº’åºåˆ—ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªç‰©å“çš„SID

è¾“å…¥æ–‡ä»¶:
- kling_items.json: ç‰©å“å…ƒæ•°æ®
- kling_sequential.txt: ç”¨æˆ·è¡Œä¸ºåºåˆ—

è¾“å‡ºæ–‡ä»¶:
- training_prediction_sid_data_train.parquet
- training_prediction_sid_data_val.parquet
- training_prediction_sid_data_test.parquet
"""

from __future__ import annotations

import json
from pathlib import Path

import pandas as pd


def load_kling_items(items_file: Path) -> dict:
    """åŠ è½½Klingç‰©å“å…ƒæ•°æ®"""
    print(f"åŠ è½½ç‰©å“å…ƒæ•°æ®: {items_file}")
    with items_file.open("r", encoding="utf-8") as f:
        items = json.load(f)
    print(f"ç‰©å“æ•°é‡: {len(items):,}")
    return items


def extract_sid_sequence(
    item_ids: list[str], 
    user_id: str, 
    kling_items: dict
) -> list[str]:
    """
    æå–ç‰©å“IDåºåˆ—å¯¹åº”çš„SIDåºåˆ—
    
    Args:
        item_ids: ç‰©å“IDåˆ—è¡¨
        user_id: ç”¨æˆ·ID
        kling_items: ç‰©å“å…ƒæ•°æ®å­—å…¸
    
    Returns:
        SIDå­—ç¬¦ä¸²åˆ—è¡¨
    """
    sid_sequence: list[str] = []
    
    for item_id in item_ids:
        item_info = kling_items.get(item_id)
        if not item_info:
            continue
        
        sid = item_info.get("sid")
        if not sid:
            continue
        
        sid_sequence.append(sid)
    
    return sid_sequence


def build_dataset_entry(
    user_id: str,
    sid_sequence: list[str],
    tail_remove_count: int,
) -> dict | None:
    """
    æ„å»ºå•ä¸ªè®­ç»ƒæ ·æœ¬
    
    Args:
        user_id: ç”¨æˆ·ID
        sid_sequence: SIDåºåˆ—
        tail_remove_count: å°¾éƒ¨ç§»é™¤æ•°é‡ï¼ˆtrain=2, val=1, test=0ï¼‰
    
    Returns:
        è®­ç»ƒæ ·æœ¬å­—å…¸
    """
    # åºåˆ—é•¿åº¦æ£€æŸ¥
    if len(sid_sequence) <= tail_remove_count + 1:
        return None
    
    # åˆ†å‰²åºåˆ—
    candidate_sequence = (
        sid_sequence[: len(sid_sequence) - tail_remove_count]
        if tail_remove_count > 0
        else sid_sequence
    )
    
    if len(candidate_sequence) < 2:
        return None
    
    # groundtruthæ˜¯æœ€åä¸€ä¸ªSID
    groundtruth = candidate_sequence[-1]
    description_sids = candidate_sequence[:-1]
    
    if not description_sids:
        return None
    
    # æ„å»ºæè¿°ï¼šç®€å•çš„SIDåºåˆ—
    description = "The user has interacted with the following items: " + "; ".join(description_sids) + ";"
    
    return {
        "user_id": user_id,
        "description": description,
        "groundtruth": groundtruth,
    }


def generate_kling_sid_prediction_data(
    sequential_file: Path,
    items_file: Path,
    output_train: Path,
    output_val: Path,
    output_test: Path,
) -> None:
    """ç”ŸæˆKling SIDé¢„æµ‹è®­ç»ƒæ•°æ®"""
    
    # åŠ è½½æ•°æ®
    kling_items = load_kling_items(items_file)
    
    print(f"\nåŠ è½½ç”¨æˆ·åºåˆ—æ–‡ä»¶: {sequential_file}")
    with sequential_file.open("r", encoding="utf-8") as f:
        lines = f.readlines()
    print(f"åºåˆ—æ•°é‡: {len(lines):,}")
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    print("\nç”Ÿæˆè®­ç»ƒæ•°æ®...")
    train_rows: list[dict] = []
    val_rows: list[dict] = []
    test_rows: list[dict] = []
    
    skipped_users = 0
    valid_users = 0
    
    for idx, line in enumerate(lines):
        line = line.strip()
        if not line:
            continue
        
        elements = line.split()
        if len(elements) <= 1:
            continue
        
        user_id = elements[0]
        item_ids = elements[1:]
        
        # æå–SIDåºåˆ—
        sid_sequence = extract_sid_sequence(item_ids, user_id, kling_items)
        if len(sid_sequence) == 0:
            skipped_users += 1
            continue
        
        valid_users += 1
        
        # ç”Ÿæˆtrain/val/testæ ·æœ¬
        entry_train = build_dataset_entry(user_id, sid_sequence, tail_remove_count=2)
        if entry_train:
            train_rows.append(entry_train)
        
        entry_val = build_dataset_entry(user_id, sid_sequence, tail_remove_count=1)
        if entry_val:
            val_rows.append(entry_val)
        
        entry_test = build_dataset_entry(user_id, sid_sequence, tail_remove_count=0)
        if entry_test:
            test_rows.append(entry_test)
        
        if (idx + 1) % 1000 == 0:
            print(f"  å·²å¤„ç† {idx + 1:,} ä¸ªç”¨æˆ·...")
    
    print(f"\nå¤„ç†å®Œæˆ:")
    print(f"  æœ‰æ•ˆç”¨æˆ·: {valid_users:,}")
    print(f"  è·³è¿‡ç”¨æˆ·: {skipped_users:,} (æ— æœ‰æ•ˆSID)")
    
    # åˆ›å»ºDataFrame
    print("\nåˆ›å»ºDataFrame...")
    df_train = pd.DataFrame(train_rows)
    df_val = pd.DataFrame(val_rows)
    df_test = pd.DataFrame(test_rows)
    
    print(f"\nâœ… æ•°æ®é›†å¤§å°:")
    print(f"  è®­ç»ƒé›†: {len(df_train):,} æ¡")
    print(f"  éªŒè¯é›†: {len(df_val):,} æ¡")
    print(f"  æµ‹è¯•é›†: {len(df_test):,} æ¡")
    
    # ä¿å­˜æ–‡ä»¶
    print(f"\nä¿å­˜è®­ç»ƒé›†åˆ°: {output_train}")
    df_train.to_parquet(output_train, engine="pyarrow", index=False)
    
    print(f"ä¿å­˜éªŒè¯é›†åˆ°: {output_val}")
    df_val.to_parquet(output_val, engine="pyarrow", index=False)
    
    print(f"ä¿å­˜æµ‹è¯•é›†åˆ°: {output_test}")
    df_test.to_parquet(output_test, engine="pyarrow", index=False)
    
    # é¢„è§ˆæ•°æ®
    def preview(df: pd.DataFrame, name: str) -> None:
        if len(df) == 0:
            return
        print(f"\nğŸ“‹ {name} å‰2æ¡æ ·æœ¬é¢„è§ˆ:")
        for i, (_, row) in enumerate(df.head(2).iterrows()):
            print(f"\n  æ ·æœ¬ {i+1}:")
            print(f"    ç”¨æˆ·ID: {row['user_id']}")
            print(f"    æè¿°: {row['description'][:200]}...")
            print(f"    é¢„æµ‹ç›®æ ‡: {row['groundtruth']}")
    
    preview(df_train, "è®­ç»ƒé›†")
    preview(df_val, "éªŒè¯é›†")
    
    print("\nâœ¨ æ•°æ®ç”Ÿæˆå®Œæˆ!")


if __name__ == "__main__":
    # æ–‡ä»¶è·¯å¾„
    sequential_file = Path("./kling_sequential.txt")
    items_file = Path("./kling_items.json")
    
    output_train = Path("./training_prediction_sid_data_train.parquet")
    output_val = Path("./training_prediction_sid_data_val.parquet")
    output_test = Path("./training_prediction_sid_data_test.parquet")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    for file in [sequential_file, items_file]:
        if not file.exists():
            print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {file}")
            exit(1)
    
    # ç”Ÿæˆæ•°æ®
    generate_kling_sid_prediction_data(
        sequential_file=sequential_file,
        items_file=items_file,
        output_train=output_train,
        output_val=output_val,
        output_test=output_test,
    )
