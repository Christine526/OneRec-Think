#!/usr/bin/env python3
"""
Klingæ•°æ®é¢„å¤„ç†è„šæœ¬
å°†kling_data.tsvè½¬æ¢ä¸ºOneRec-Thinkéœ€è¦çš„æ ¼å¼

ç”Ÿæˆä¸‰ä¸ªæ ¸å¿ƒæ–‡ä»¶ï¼š
1. kling_items.json - ç‰©å“å…ƒæ•°æ®ï¼ˆtitle, description, categories, sidï¼‰
2. kling_sequential.txt - ç”¨æˆ·è¡Œä¸ºåºåˆ—ï¼ˆuser_id + item_idsï¼‰
3. kling_user_behaviors.json - ç”¨æˆ·è¡Œä¸ºè¯¦æƒ…ï¼ˆåŒ…å«è¡Œä¸ºç±»å‹ã€æœç´¢è¯ç­‰å¯Œæ–‡æœ¬ä¿¡æ¯ï¼‰
"""

import pandas as pd
import json
from pathlib import Path
import ast
import warnings
warnings.filterwarnings('ignore')


def clean_column_names(df):
    """æ¸…ç†åˆ—åï¼Œç§»é™¤å‰ç¼€"""
    df.columns = [col.split('.')[-1] for col in df.columns]
    return df


def parse_semantic_id(semantic_id_str):
    """
    è§£æsemantic_idå­—ç¬¦ä¸²ä¸ºæ•°ç»„
    ä¾‹å¦‚: ' [69,142,246] ' -> [69, 142, 246]
    """
    if pd.isna(semantic_id_str) or semantic_id_str == '':
        return None
    try:
        # å»é™¤ç©ºæ ¼å¹¶è§£æ
        semantic_id_str = str(semantic_id_str).strip()
        return ast.literal_eval(semantic_id_str)
    except:
        return None


def format_semantic_id_to_sid(semantic_id_array):
    """
    å°†semantic_idæ•°ç»„è½¬æ¢ä¸ºSIDæ ¼å¼
    Klingæ•°æ®æ˜¯3ç»´ï¼ŒBeautyæ•°æ®æ˜¯4ç»´
    ä¾‹å¦‚: [69, 142, 246] -> <|sid_begin|><s_a_69><s_b_142><s_c_246><|sid_end|>
    """
    if not semantic_id_array or len(semantic_id_array) == 0:
        return None
    
    prefixes = ['s_a', 's_b', 's_c']  # 3ç»´ï¼Œé€‚é…klingæ•°æ®
    sid_tokens = []
    
    for i, val in enumerate(semantic_id_array[:3]):  # æœ€å¤šå–3ä¸ª
        if i < len(prefixes):
            sid_tokens.append(f'<{prefixes[i]}_{val}>')
    
    return '<|sid_begin|>' + ''.join(sid_tokens) + '<|sid_end|>'


def extract_category(row):
    """
    ä»æ•°æ®ä¸­æå–ç±»åˆ«ä¿¡æ¯
    content_type: 0=å›¾ç‰‡+è§†é¢‘, 1=å›¾ç‰‡, 2=è§†é¢‘
    kling_photo_type: 1=ç´ æ, 2=çŸ­ç‰‡
    """
    # åŸºäºcontent_type
    if pd.notna(row.get('content_type')):
        content_type_map = {
            '0': 'Image and Video Creation',
            '1': 'Image Creation',
            '2': 'Video Creation'
        }
        content_type = str(int(row['content_type']))  # è½¬ä¸ºæ•´æ•°å†è½¬å­—ç¬¦ä¸²
        category = content_type_map.get(content_type, f'Content Type {content_type}')
    else:
        category = 'General Creation'
    
    # æ·»åŠ photo_typeä¿¡æ¯
    if pd.notna(row.get('kling_photo_type')):
        photo_type = int(row['kling_photo_type'])
        if photo_type == 1:
            category += ' > Material'  # ç´ æ
        elif photo_type == 2:
            category += ' > Short Film'  # çŸ­ç‰‡
        else:
            category += f' > Type {photo_type}'
    
    return category


def create_item_metadata(df_raw, output_path):
    """
    åˆ›å»ºç‰©å“å…ƒæ•°æ®JSON (ç±»ä¼¼Beauty.pretrain.json)
    
    æ ¼å¼ï¼š
    {
        "item_id": {
            "title": "ç‰©å“æ ‡é¢˜",
            "description": "ç‰©å“æè¿°(prompt)",
            "categories": "ç±»åˆ«å±‚çº§",
            "sid": "<|sid_begin|><s_a_X><s_b_Y><s_c_Z><|sid_end|>"
        },
        ...
    }
    """
    print("\n" + "="*60)
    print("ğŸ“Š ç»Ÿè®¡åŸå§‹æ•°æ®ä¿¡æ¯")
    print("="*60)
    
    # ç»Ÿè®¡è¡Œä¸ºç±»å‹åˆ†å¸ƒï¼ˆevent_type: RECOMMEND/SEARCH/PRODUCEï¼‰
    if 'event_type' in df_raw.columns:
        print("\nğŸ“ˆ è¡Œä¸ºç±»å‹åˆ†å¸ƒ (event_type):")
        event_counts = df_raw['event_type'].value_counts()
        
        # æŒ‰ç±»å‹æ’åºæ˜¾ç¤º
        event_order = ['RECOMMEND', 'SEARCH', 'PRODUCE']
        for event_type in event_order:
            if event_type in event_counts:
                count = event_counts[event_type]
                pct = count/len(df_raw)*100
                print(f"  {event_type:12s}: {count:6,d} æ¡ ({pct:5.1f}%)")
    
    # ç»Ÿè®¡æœç´¢æŸ¥è¯¢å†…å®¹ï¼ˆä»…ç»Ÿè®¡å±•ç¤ºï¼Œæš‚æœªç”¨äºè®­ç»ƒï¼‰
    if 'element_query_content' in df_raw.columns:
        search_with_query = df_raw['element_query_content'].notna().sum()
        total_search = (df_raw['event_type'] == 'SEARCH').sum() if 'event_type' in df_raw.columns else 0
        
        print("\nğŸ” æœç´¢åœºæ™¯ç»Ÿè®¡ (element_query_content):")
        if total_search > 0:
            print(f"  SEARCHç±»å‹æ€»æ•°: {total_search:,} æ¡")
            print(f"  åŒ…å«æŸ¥è¯¢è¯: {search_with_query:,} æ¡ ({search_with_query/total_search*100:.1f}%)")
        else:
            print(f"  åŒ…å«æŸ¥è¯¢è¯çš„è®°å½•: {search_with_query:,} æ¡")
    
    print("\n" + "="*60)
    print("ğŸ”§ å¤„ç†ç‰©å“å…ƒæ•°æ®")
    print("="*60)
    
    # æŒ‰kling_photo_idå»é‡ï¼Œä¿ç•™æœ€æ–°çš„è®°å½•
    df_items = df_raw.sort_values('time_stamp').drop_duplicates(
        subset=['kling_photo_id'], keep='last'
    )
    
    print(f"å»é‡å‰: {len(df_raw):,} æ¡è®°å½•")
    print(f"å»é‡å: {len(df_items):,} ä¸ªå”¯ä¸€ç‰©å“")
    
    items_dict = {}
    missing_sid_count = 0
    
    for idx, row in df_items.iterrows():
        item_id = str(row['kling_photo_id'])
        
        # è§£æsemantic_id
        semantic_id_array = parse_semantic_id(row['semantic_id'])
        if not semantic_id_array:
            missing_sid_count += 1
            continue
        
        # è½¬æ¢ä¸ºSIDæ ¼å¼
        sid = format_semantic_id_to_sid(semantic_id_array)
        if not sid:
            missing_sid_count += 1
            continue
        
        # æå–ç±»åˆ«
        categories = extract_category(row)
        
        # ä½¿ç”¨promptä½œä¸ºdescriptionï¼Œé™åˆ¶é•¿åº¦é¿å…è¶…é•¿æ–‡æœ¬
        description = row['prompt'] if pd.notna(row['prompt']) else ''
        if len(description) > 1000:  # é™åˆ¶å­—ç¬¦æ•°ï¼ŒQWençš„tokené™åˆ¶ä¸ºmax_length=4096
            description = description[:1000] + '...'
        
        # ä½¿ç”¨titleæˆ–introductionä½œä¸ºæ ‡é¢˜
        title = row['title'] if pd.notna(row['title']) else ''
        if not title and pd.notna(row['introduction']):
            title = row['introduction'][:100]  # æˆªå–å‰100å­—ç¬¦
        if not title:
            title = description[:100] if description else f'Item {item_id}'
        
        items_dict[item_id] = {
            'title': title,
            'description': description,
            'categories': categories,
            'sid': sid
        }
    
    print(f"æˆåŠŸå¤„ç†: {len(items_dict):,} ä¸ªç‰©å“")
    print(f"ç¼ºå¤±SID: {missing_sid_count:,} ä¸ªç‰©å“")
    
    # ä¿å­˜ä¸ºJSON
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(items_dict, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç‰©å“å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    
    return items_dict


def create_sequential_file(df_raw, output_path, min_sequence_length=3):
    """
    åˆ›å»ºç”¨æˆ·åºåˆ—æ–‡ä»¶ (ç±»ä¼¼sequential_data_processed.txt)
    
    æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªç”¨æˆ·çš„è¡Œä¸ºåºåˆ—
    user_id item_id1 item_id2 item_id3 ...
    
    æ³¨æ„: å‡è®¾CSVä¸­å·²ç»åŒ…å«æ­£å‘è¡Œä¸ºï¼ˆåœ¨SQLä¸­å·²ç­›é€‰ï¼‰
    
    Args:
        df_raw: æ­£å‘è¡Œä¸ºæ•°æ®DataFrame
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        min_sequence_length: æœ€å°åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤3ï¼Œç”¨äºåç»­è®­ç»ƒåˆ†å‰²ï¼‰
    """
    print("\n" + "="*60)
    print("ğŸ“ åˆ›å»ºç”¨æˆ·è¡Œä¸ºåºåˆ—æ–‡ä»¶")
    print("="*60)
    print(f"æ€»è®°å½•æ•°: {len(df_raw):,} æ¡")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„semantic_id
    df_raw['semantic_id_parsed'] = df_raw['semantic_id'].apply(parse_semantic_id)
    df_positive = df_raw[df_raw['semantic_id_parsed'].notna()].copy()
    
    # æŒ‰ç”¨æˆ·å’Œæ—¶é—´æ’åº
    df_sorted = df_positive.sort_values(['user_id', 'time_stamp'])
    
    # ç»Ÿè®¡ä¿¡æ¯
    user_counts = df_sorted.groupby('user_id').size()
    total_users = len(user_counts)
    valid_users = (user_counts >= min_sequence_length).sum()
    
    print(f"æ€»ç”¨æˆ·æ•°: {total_users:,}")
    print(f"æœ‰æ•ˆç”¨æˆ·æ•° (è¡Œä¸ºâ‰¥{min_sequence_length}): {valid_users:,}")
    
    # ç»Ÿè®¡åºåˆ—é•¿åº¦åˆ†å¸ƒ
    print("\nğŸ“Š åºåˆ—é•¿åº¦åˆ†å¸ƒ:")
    length_bins = [0, 3, 5, 10, 20, 50, float('inf')]
    length_labels = ['1-2', '3-4', '5-9', '10-19', '20-49', '50+']
    user_counts_series = user_counts.reset_index(name='count')['count']
    length_dist = pd.cut(user_counts_series, bins=length_bins, labels=length_labels, right=False)
    for label, count in length_dist.value_counts().sort_index().items():
        pct = count / total_users * 100
        print(f"  {label:6s}: {count:6,d} ç”¨æˆ· ({pct:5.1f}%)")
    
    # å†™å…¥åºåˆ—æ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        for user_id, group in df_sorted.groupby('user_id'):
            item_ids = group['kling_photo_id'].astype(str).tolist()
            if len(item_ids) >= min_sequence_length:
                line = f"{user_id} {' '.join(item_ids)}\n"
                f.write(line)
    
    print(f"âœ… ç”¨æˆ·åºåˆ—æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_path}")
    
    return valid_users


def create_user_behaviors_file(df_raw, output_path):
    """
    åˆ›å»ºç”¨æˆ·è¡Œä¸ºè¯¦ç»†ä¿¡æ¯æ–‡ä»¶
    åŒ…å«æ¯ä¸ªäº¤äº’çš„è¡Œä¸ºç±»å‹ã€æœç´¢è¯ç­‰å¯Œæ–‡æœ¬ä¿¡æ¯
    
    è¿™ä¸ªæ–‡ä»¶ç”¨äºç”Ÿæˆå¯Œæ–‡æœ¬çš„Alignmentè®­ç»ƒæ•°æ®(Interleaved User Persona Grounding)
    
    æ ¼å¼ï¼š
    {
        "user_id": [
            {
                "item_id": "xxx",
                "event_type": "RECOMMEND/SEARCH/PRODUCE",
                "behavior_type": "OPERATE/VIDEO_PLAY_FINISH/...",
                "behavior_subtype": "LIKE/COMMENT/...",
                "timestamp": "...",
                "element_query_content": "æœç´¢è¯",
                "query_cnt": æœç´¢æ¬¡æ•°
            },
            ...
        ],
        ...
    }
    
    Args:
        df_raw: åŸå§‹è¡Œä¸ºæ•°æ®DataFrame
        output_path: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
    """
    print("\n" + "="*60)
    print("ğŸ“‹ åˆ›å»ºç”¨æˆ·è¡Œä¸ºè¯¦æƒ…æ–‡ä»¶")
    print("="*60)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„semantic_id
    df_raw['semantic_id_parsed'] = df_raw['semantic_id'].apply(parse_semantic_id)
    df_positive = df_raw[df_raw['semantic_id_parsed'].notna()].copy()
    
    # æŒ‰ç”¨æˆ·å’Œæ—¶é—´æ’åº
    df_sorted = df_positive.sort_values(['user_id', 'time_stamp'])
    
    user_behaviors = {}
    
    for user_id, group in df_sorted.groupby('user_id'):
        behaviors = []
        
        for _, row in group.iterrows():
            behavior_info = {
                'item_id': str(row['kling_photo_id']),
                'event_type': row['event_type'] if pd.notna(row.get('event_type')) else 'UNKNOWN',
                'behavior_type': row['behavior_type'] if pd.notna(row.get('behavior_type')) else '',
                'behavior_subtype': row['behavior_subtype'] if pd.notna(row.get('behavior_subtype')) else '',
                'timestamp': str(row['time_stamp']) if pd.notna(row.get('time_stamp')) else '',
                'element_query_content': row['element_query_content'] if pd.notna(row.get('element_query_content')) else '',
                'query_cnt': int(row['query_cnt']) if pd.notna(row.get('query_cnt')) else 0
            }
            behaviors.append(behavior_info)
        
        user_behaviors[str(user_id)] = behaviors
    
    # ä¿å­˜ä¸ºJSON
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(user_behaviors, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç”¨æˆ·è¡Œä¸ºè¯¦æƒ…å·²ä¿å­˜åˆ°: {output_path}")
    print(f"æ€»ç”¨æˆ·æ•°: {len(user_behaviors):,}")
    
    return user_behaviors


def main():
    print("="*60)
    print("ğŸš€ Klingæ•°æ®é¢„å¤„ç† - ç”ŸæˆOneRec-Thinkè®­ç»ƒå…ƒæ•°æ®")
    print("="*60)
    
    # æ–‡ä»¶è·¯å¾„ï¼ˆæŒ‰ä¼˜å…ˆçº§æ£€æŸ¥ï¼‰
    possible_files = [
        Path("./kling_data_fixed.tsv"),  # ä¿®å¤åçš„TSVï¼ˆä¼˜å…ˆï¼‰
        Path("./kling_data.tsv"),        # TSVæ ¼å¼ï¼ˆåˆ¶è¡¨ç¬¦åˆ†éš”ï¼‰
        Path("./kling_data_fixed.csv"),  # ä¿®å¤åçš„CSVï¼ˆå¤‡é€‰ï¼‰
        Path("./kling_data.csv"),        # CSVæ ¼å¼ï¼ˆé€—å·åˆ†éš”ï¼Œå¤‡é€‰ï¼‰
    ]
    
    input_file = None
    for f in possible_files:
        if f.exists():
            input_file = f
            print(f"âœ“ æ£€æµ‹åˆ°æ•°æ®æ–‡ä»¶: {input_file}\n")
            break
    
    items_output = Path("./kling_items.json")
    sequential_output = Path("./kling_sequential.txt")
    behaviors_output = Path("./kling_user_behaviors.json")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if input_file is None:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ï¼Œè¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶ä¹‹ä¸€å­˜åœ¨:")
        for f in possible_files:
            print(f"  - {f}")
        print("\næç¤º: è¯·å…ˆè¿è¡Œ load_kling_data.py ä»Hiveè·å–æ•°æ®")
        return
    
    # è¯»å–CSV (å°è¯•å¤šç§åˆ†éš”ç¬¦å’Œç¼–ç )
    print(f"ğŸ“¥ è¯»å–æ•°æ®æ–‡ä»¶: {input_file}")
    df = None
    
    # å°è¯•å¤šç§è¯»å–æ–¹å¼
    attempts = [
        # 1. åˆ¶è¡¨ç¬¦åˆ†éš”ï¼ˆæœ€å¸¸è§çš„è§£å†³æ–¹æ¡ˆï¼‰
        {'sep': '\t', 'encoding': 'utf-8-sig', 'engine': 'python', 'desc': 'åˆ¶è¡¨ç¬¦åˆ†éš”'},
        # 2. æ ‡å‡†CSV
        {'sep': ',', 'encoding': 'utf-8-sig', 'desc': 'æ ‡å‡†CSV'},
        # 3. Pythonå¼•æ“ + åˆ¶è¡¨ç¬¦
        {'sep': '\t', 'encoding': 'utf-8-sig', 'engine': 'python', 'on_bad_lines': 'skip', 'desc': 'åˆ¶è¡¨ç¬¦(è·³è¿‡åè¡Œ)'},
        # 4. Pythonå¼•æ“ + é€—å·ï¼ˆå®½æ¾æ¨¡å¼ï¼‰
        {'sep': ',', 'encoding': 'utf-8-sig', 'engine': 'python', 'on_bad_lines': 'skip', 'quoting': 3, 'desc': 'é€—å·åˆ†éš”(è·³è¿‡åè¡Œ)'},
    ]
    
    for i, params in enumerate(attempts, 1):
        desc = params.pop('desc')
        try:
            print(f"  å°è¯•æ–¹å¼ {i}: {desc}")
            df = pd.read_csv(input_file, **params)
            print(f"  âœ“ æˆåŠŸ! è¯»å– {len(df):,} æ¡è®°å½•ï¼Œ{len(df.columns)} åˆ—")
            break
        except (pd.errors.ParserError, TypeError) as e:
            print(f"    å¤±è´¥: {str(e)[:50]}...")
            continue
        except Exception as e:
            print(f"    å¤±è´¥: {str(e)[:50]}...")
            continue
    
    # å¦‚æœæ‰€æœ‰æ–¹å¼éƒ½å¤±è´¥
    if df is None or len(df) == 0:
        print(f"\nâŒ é”™è¯¯: æ— æ³•è¯»å–æ•°æ®æ–‡ä»¶")
        print(f"å»ºè®®: è¯·ä½¿ç”¨åˆ¶è¡¨ç¬¦(\\t)é‡æ–°å¯¼å‡ºæ•°æ®")
        return
    
    # ç§»é™¤BOMï¼ˆå¦‚æœæœ‰ï¼‰
    if df.columns[0].startswith('\ufeff'):
        df.columns = [col.replace('\ufeff', '') for col in df.columns]
        print(f"  âœ“ ç§»é™¤äº†UTF-8 BOMæ ‡è®°")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¡¨å¤´ï¼ˆå¦‚æœç¬¬ä¸€è¡Œå…¨æ˜¯æ•°å­—ï¼Œè¯´æ˜æ²¡æœ‰è¡¨å¤´ï¼‰
    first_row = df.iloc[0]
    if all(str(val).replace('.', '').replace('-', '').replace(':', '').replace(' ', '').isdigit() or pd.isna(val) for val in first_row[:3]):
        print(f"  âš ï¸  æ£€æµ‹åˆ°æ–‡ä»¶å¯èƒ½æ²¡æœ‰è¡¨å¤´ï¼Œè®¾ç½®æ ‡å‡†åˆ—å...")
        expected_columns = [
            'user_id', 'kling_photo_id', 'kling_photo_type', 'event_type',
            'behavior_type', 'behavior_subtype', 'time_stamp', 'content_type',
            'prompt', 'title', 'introduction', 'element_query_content',
            'query_cnt', 'semantic_id'
        ]
        if len(df.columns) == len(expected_columns):
            df.columns = expected_columns
            print(f"  âœ“ å·²è®¾ç½®æ ‡å‡†åˆ—å")
        else:
            print(f"  âš ï¸  åˆ—æ•°ä¸åŒ¹é…: æœŸæœ› {len(expected_columns)}ï¼Œå®é™… {len(df.columns)}")
    
    # æ¸…ç†åˆ—å
    df = clean_column_names(df)
    
    # æ­¥éª¤1: åˆ›å»ºç‰©å“å…ƒæ•°æ®
    items_dict = create_item_metadata(df, items_output)
    
    # æ­¥éª¤2: åˆ›å»ºç”¨æˆ·åºåˆ—
    valid_users = create_sequential_file(df, sequential_output)
    
    # æ­¥éª¤3: åˆ›å»ºç”¨æˆ·è¡Œä¸ºè¯¦æƒ…ï¼ˆç”¨äºç”Ÿæˆå¯Œæ–‡æœ¬Alignmentæ•°æ®ï¼‰
    user_behaviors = create_user_behaviors_file(df, behaviors_output)
    
    print(f"\n" + "="*60)
    print(f"âœ… æ•°æ®å¤„ç†å®Œæˆ!")
    print(f"="*60)
    print(f"ğŸ“¦ è¾“å‡ºæ–‡ä»¶:")
    print(f"  1. {items_output} ({len(items_dict):,} ä¸ªç‰©å“)")
    print(f"  2. {sequential_output} ({valid_users:,} ä¸ªç”¨æˆ·)")
    print(f"  3. {behaviors_output} ({len(user_behaviors):,} ä¸ªç”¨æˆ·)")
    print(f"\nğŸ“š ä¸‹ä¸€æ­¥: è¿è¡Œä»¥ä¸‹è„šæœ¬ç”Ÿæˆè®­ç»ƒæ•°æ®")
    print(f"  - generate_kling_RA_data.py (Alignmentæ•°æ®)")
    print(f"  - generate_kling_sid_prediction_data.py (SIDé¢„æµ‹æ•°æ®)")
    print(f"  - generate_kling_training_data.py (Dense Captioningæ•°æ®)")


if __name__ == "__main__":
    main()
