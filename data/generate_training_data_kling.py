#!/usr/bin/env python3
"""
Klingæ•°æ® - Alignmentè®­ç»ƒæ•°æ®ç”Ÿæˆè„šæœ¬

ç”¨é€”: Stage 1 è®­ç»ƒ - Itemix Alignment
è®©æ¨¡å‹å­¦ä¹ ç‰©å“çš„SIDã€æ ‡é¢˜å’Œç±»åˆ«ä¹‹é—´çš„æ˜ å°„å…³ç³»

è¾“å…¥:
  - kling_items.json (ç‰©å“å…ƒæ•°æ®, 4ç»´SID)
  - kling_sequential.txt (ç”¨æˆ·åºåˆ—)

è¾“å‡º:
  - training_align_data_kling_train.parquet
  - training_align_data_kling_val.parquet
  - training_align_data_kling_test.parquet

æ•°æ®æ ¼å¼:
  {
      'user_id': '123',
      'description': 'The user has purchased the following items: <|sid_begin|>...<|sid_end|>, its title is "...", its categories are "..."; ...'
  }
"""

import json
import pandas as pd
from pathlib import Path


def generate_training_data(sequential_file, items_file, output_train_file, output_val_file, output_test_file):
    """ç”ŸæˆAlignmentè®­ç»ƒæ•°æ®ï¼ˆStage 1ï¼‰"""
    
    print("="*60)
    print("ğŸ¯ ç”ŸæˆKling Alignmentè®­ç»ƒæ•°æ® (Stage 1)")
    print("="*60)
    
    try:
        # åŠ è½½ç‰©å“å…ƒæ•°æ®
        print(f"\nğŸ“– åŠ è½½ç‰©å“å…ƒæ•°æ®: {items_file}")
        with open(items_file, 'r', encoding='utf-8') as f:
            items = json.load(f)
        print(f"   âœ… åŠ è½½äº† {len(items)} ä¸ªç‰©å“")
        
        # éªŒè¯SIDæ ¼å¼ï¼ˆåº”è¯¥æ˜¯4ç»´ï¼‰
        sample_item = list(items.values())[0]
        sample_sid = sample_item['sid']
        has_4_dims = all(f'<s_{dim}_' in sample_sid for dim in ['a', 'b', 'c', 'd'])
        if not has_4_dims:
            print(f"   âš ï¸  è­¦å‘Š: SIDæ ¼å¼å¯èƒ½ä¸æ­£ç¡®ï¼ˆåº”è¯¥æ˜¯4ç»´ï¼‰")
            print(f"   ç¤ºä¾‹: {sample_sid}")
        else:
            print(f"   âœ… SIDæ ¼å¼éªŒè¯é€šè¿‡ï¼ˆ4ç»´ï¼‰")

        # åŠ è½½ç”¨æˆ·åºåˆ—
        print(f"\nğŸ“– åŠ è½½ç”¨æˆ·åºåˆ—: {sequential_file}")
        with open(sequential_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        print(f"   âœ… åŠ è½½äº† {len(lines)} ä¸ªç”¨æˆ·åºåˆ—")

        training_data_train = []
        training_data_val = []
        training_data_test = []
        missing_items_count = 0

        def build_description(item_ids, user_id):
            """æ„å»ºç‰©å“æè¿°æ–‡æœ¬"""
            item_descriptions = []
            local_missing_count = 0

            for item_id in item_ids:
                item_info = items.get(item_id)
                if not item_info:
                    local_missing_count += 1
                    continue

                sid = item_info.get('sid', '')
                title = item_info.get('title', '')
                categories = item_info.get('categories', '')

                if sid and title and categories:
                    # æ ¼å¼: SID + title + categories
                    item_desc = f'{sid}, its title is "{title}", its categories are "{categories}"'
                    item_descriptions.append(item_desc)
                else:
                    local_missing_count += 1

            return item_descriptions, local_missing_count

        print(f"\nğŸ”„ å¤„ç†ç”¨æˆ·åºåˆ—...")
        processed_count = 0
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue

            elements = line.split()
            if len(elements) <= 1:
                continue

            user_id = elements[0]
            item_ids = elements[1:]

            # Testé›†: ä½¿ç”¨å®Œæ•´åºåˆ—
            full_item_descriptions, missing_count_full = build_description(item_ids, user_id)
            missing_items_count += missing_count_full

            if full_item_descriptions:
                test_description = "The user has purchased the following items: " + "; ".join(full_item_descriptions) + ";"
                training_data_test.append({
                    'user_id': user_id,
                    'description': test_description
                })

            # Valé›†: å»æ‰æœ€å1ä¸ªitem
            if len(item_ids) > 1:
                val_item_ids = item_ids[:-1]
                val_item_descriptions, missing_count_val = build_description(val_item_ids, user_id)
                missing_items_count += missing_count_val

                if val_item_descriptions:
                    val_description = "The user has purchased the following items: " + "; ".join(val_item_descriptions) + ";"
                    training_data_val.append({
                        'user_id': user_id,
                        'description': val_description
                    })

            # Trainé›†: å»æ‰æœ€å2ä¸ªitem
            if len(item_ids) > 2:
                train_item_ids = item_ids[:-2]
                train_item_descriptions, missing_count_train = build_description(train_item_ids, user_id)
                missing_items_count += missing_count_train

                if train_item_descriptions:
                    train_description = "The user has purchased the following items: " + "; ".join(train_item_descriptions) + ";"
                    training_data_train.append({
                        'user_id': user_id,
                        'description': train_description
                    })
            
            processed_count += 1
            if processed_count % 1000 == 0:
                print(f"   å·²å¤„ç† {processed_count}/{len(lines)} ä¸ªç”¨æˆ·...")

        print(f"   âœ… å®Œæˆå¤„ç† {processed_count} ä¸ªç”¨æˆ·")
        print(f"   âš ï¸  ç¼ºå¤±ç‰©å“æ•°: {missing_items_count}")

        # åˆ›å»ºDataFrame
        print(f"\nğŸ“Š åˆ›å»ºDataFrame...")
        df_train = pd.DataFrame(training_data_train)
        df_val = pd.DataFrame(training_data_val)
        df_test = pd.DataFrame(training_data_test)

        print(f"   Trainé›†: {len(df_train)} æ¡")
        print(f"   Valé›†:   {len(df_val)} æ¡")
        print(f"   Testé›†:  {len(df_test)} æ¡")

        # ä¿å­˜parquetæ–‡ä»¶
        print(f"\nğŸ’¾ ä¿å­˜parquetæ–‡ä»¶...")
        df_train.to_parquet(output_train_file, engine='pyarrow', index=False)
        print(f"   âœ… {output_train_file}")
        
        df_val.to_parquet(output_val_file, engine='pyarrow', index=False)
        print(f"   âœ… {output_val_file}")
        
        df_test.to_parquet(output_test_file, engine='pyarrow', index=False)
        print(f"   âœ… {output_test_file}")

        # æ˜¾ç¤ºç¤ºä¾‹
        print(f"\nğŸ“ æ•°æ®ç¤ºä¾‹ (Trainé›†å‰2æ¡):")
        for i, row in df_train.head(2).iterrows():
            print(f"\n   [{i+1}] User: {row['user_id']}")
            desc = row['description']
            if len(desc) > 200:
                print(f"       Description: {desc[:200]}...")
            else:
                print(f"       Description: {desc}")
        
        print(f"\n{'='*60}")
        print(f"âœ… Alignmentè®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆ!")
        print(f"{'='*60}")
        print(f"\nğŸ¯ ä¸‹ä¸€æ­¥:")
        print(f"   è¿è¡Œè®­ç»ƒè„šæœ¬: train/run_training_stage1.sh")
        print(f"   æˆ–ä½¿ç”¨: train/scripts/train_beauty_align.py\n")

    except Exception as e:
        print(f"\nâŒ ç”Ÿæˆè®­ç»ƒæ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # é»˜è®¤è·¯å¾„ï¼ˆå¯æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
    sequential_file = "../kling_data/kling_sequential.txt"
    items_file = "../kling_data/kling_items.json"
    
    output_train_file = "./training_align_data_kling_train.parquet"
    output_val_file = "./training_align_data_kling_val.parquet"
    output_test_file = "./training_align_data_kling_test.parquet"
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(sequential_file).exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {sequential_file}")
        print(f"   è¯·å…ˆè¿è¡Œ: kling_data/process_kling_data_fixed.py")
        exit(1)
    
    if not Path(items_file).exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {items_file}")
        print(f"   è¯·å…ˆè¿è¡Œ: kling_data/process_kling_data_fixed.py")
        exit(1)

    generate_training_data(
        sequential_file,
        items_file,
        output_train_file,
        output_val_file,
        output_test_file
    )
